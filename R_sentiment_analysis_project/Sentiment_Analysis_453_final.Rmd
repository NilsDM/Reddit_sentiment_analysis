---
title: "Sentiment_Analysis"
author: "Nils Dosaj Mikkelsen (V00901004)"
date: "15/03/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries

```{r, warning=FALSE, message=FALSE}
library(tidyr)  
library(tibble)
library(stringr) 
library(purrr)
library(dplyr) 
library(ggplot2)
library(gridExtra)
library(ggridges)
library(tidyquant)
library(syuzhet)
```

# Functions

```{r}

# Sentiment score of a given comment
calculate_sentiment_score <- function(comment){
    comment <- gsub("[\r\n]"," ", comment)
    comment <- gsub("[\r\n\n]","", comment)
    comment <- str_replace_all(comment, "[^[:alnum:]]", " ")
    sd <- get_nrc_sentiment(comment)
    num_score <- (sd$positive - sd$negative) / (sd$positive + sd$negative)
    return(round(num_score, 3))
}


# Bins a given sentiment score into 5 discrete bins
sentiment_bin <- function(s){
    if(-1 <= s & s <= -.5){
        return("Negative")
    }

    if(-.5 < s & s < -.1){
        return("Neg-Neutral")
    }

    if(-.1 <= s & s <= .1){
        return("Neutral")
    }

    if(.1 < s & s < .5){
        return("Pos-Neutral")
    }

    if(.5 <= s & s <= 1){
        return("Positive")
    }
}

# Sample comment from given data frame and returns all attributes as a vector
sample_comments_and_sentiment_analysis <- function(df){
    blank_df <- data.frame(subreddit = character(),
                                post_name = character(),
                                karma = integer(),
                                karma_ratio = numeric(),
                                comment_author = character(),
                                comment = character(),
                                comment_karma = numeric(),
                                comment_score = numeric(),
                                id_tag = character())
    blank_df <- data.frame()
    for(i in 1:30){
        comment_score <- NaN
        while(is.nan(comment_score)){
            sample_row <- df[sample(1:nrow(df), 1, replace = FALSE), ] 
            comment = sample_row[,6]
            comment_score <- calculate_sentiment_score(comment)
        }
        result = c(sample_row$subreddit,
                   sample_row$post_name,
                   sample_row$karma,
                   sample_row$karma_ratio,
                   sample_row$comment_author,
                   comment,
                   sample_row$comment_karma,
                   comment_score,
                   sample_row$id_tag)
        names(result) <- names(blank_df)
        blank_df = rbind(blank_df, result)
    }
    return(blank_df)
}

# Calculates the number of comments in a given post
calculate_number_of_comments <- function(df){
    comment_counter <- df %>% 
        select(subreddit, post_name, comment) %>% 
        group_by(subreddit, post_name) %>% 
        mutate(comment_count = n()) %>% 
        ungroup() %>% 
        select(subreddit, post_name, comment_count) %>% 
        distinct() 
    return(comment_counter)
}

# Separate unique post list into vectorized list separated by subreddit
vectorize_posts <- function(df, sub){
    post_names_df <- df %>% 
    filter(subreddit == sub) %>% 
    arrange(desc(comment_count)) %>% 
    select(post_name) %>% 
    head(21) 
    result_vector <- as.vector(post_names_df)
    return(result_vector)
    
}

# Function for binding data frames by day
data_frame_bind <- function(blank_df, master_df, n){
    for(i in n:(n + 2)){
        blank_df <- rbind(blank_df, master_df[[i]])
    }
    return(blank_df)
}


# Count unique posts
count_unique_posts <- function(df){
    df <- df %>% 
    group_by(subreddit) %>% 
    mutate(post_count = n()) %>% 
    ungroup() %>% 
    select(subreddit, post_count) %>% 
    distinct() %>% 
    arrange(desc(post_count))
    return(df)
}

# Outlier detector function
# Source: https://stackoverflow.com/questions/33524669/labeling-outliers-of-boxplots-in-r   
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}


# Build daily dataframe
sample_daily_dfs <- function(df, df_sampled_comments){
    for(i in 1:10){
        current_df <- 
        df %>% 
        filter(subreddit == subreddits[i]) %>% 
        group_by(subreddit, post_name) %>% 
        mutate(comment_count = n()) %>% 
        ungroup() %>% 
        filter(comment_count >= 30) %>% 
        select(subreddit, post_name, karma,
               karma_ratio, comment_author,
               comment, comment_karma, id_tag)
    
        sampled_comments  <- sample_comments_and_sentiment_analysis(current_df)
        df_sampled_comments <- rbind(df_sampled_comments, sampled_comments)
    }
    return(df_sampled_comments)
}


# Feature engineering for sentiment binning and power user boolean
feature_engineering <- function(df){
    df <- df %>% 
    mutate(is_power_user = 
               ifelse(comment_author %in% power_users_vector, TRUE, FALSE), 
           sentiment_bin = map(comment_score, sentiment_bin)) %>% 
    rename(sentiment_score = comment_score)
    return(df)
}


# Plotting function 
boxplot_plotting_with_dots <- function(df, sub = "All days"){
    result <- df %>% 
    ggplot(aes(x = subreddit, y = sentiment_score, group = subreddit)) + 
    geom_boxplot(data = df, aes(fill = subreddit, alpha = 0.4),
    notch = FALSE, show.legend = FALSE, size = .3,
    colour = "black") +
    stat_summary(fun = "mean") + 
    geom_jitter(color = "black", fill = "white", size = 1, alpha = 0.15) +
    labs(
        title    = "Sentiment Score Distribution",
        subtitle = sub,
        caption  = "Note: Big dot = Mean" ,
        x        = "SubReddits",
        y        = "Sentiment"
    ) + theme_tq() + 
    theme(axis.text.x   = element_text(angle = 45, vjust = 1, 
                                       hjust=1, size = 12, face = "bold"),
          axis.title.x  = element_text(size = 12, face = "bold", vjust = 10),
          axis.title.y  = element_text(size = 12, face = "bold")) 
    return(result)
}

# Conduct model adequacy checks
model_tests <- function(df, n){
    par(mfrow =c(1,3), pty = "s")
    res <- df$residuals
    fitted <- df$fitted.values
    
    # Normality
    qqnorm(res) 
    qqline(res, col = 2)
    print(shapiro.test(res))
    
    # Constant Variance
    plot(fitted, res, ylab = "Residuals",xlab = "Fitted Values", 
         main = "Constant Variance Test") 
    abline(h = 0, col = 4)
    
    # Independence
    plot(1:n,res, ylab = "Residuals",xlab = "Order", main = "Independence Test") 
    abline(h = 0, col = 5)
}
```

\newpage

# Data Import

```{r}
# Day 1 (Wednesday)
data_day_01_1 <- read.csv(file = "../data_dumps/data_2022-03-16-00-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_01_2 <- read.csv(file = "../data_dumps/data_2022-03-16-08-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_01_3 <- read.csv(file = "../data_dumps/data_2022-03-16-16-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

# Day 2 (Thursday)
data_day_02_1 <- read.csv(file = "../data_dumps/data_2022-03-17-00-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_02_2 <- read.csv(file = "../data_dumps/data_2022-03-17-08-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_02_3 <- read.csv(file = "../data_dumps/data_2022-03-17-16-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

# Day 3 (Friday)
data_day_03_1 <- read.csv(file = "../data_dumps/data_2022-03-18-00-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_03_2 <- read.csv(file = "../data_dumps/data_2022-03-18-08-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_03_3 <- read.csv(file = "../data_dumps/data_2022-03-18-16-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

# Day 4 (Saturday)
data_day_04_1 <- read.csv(file = "../data_dumps/data_2022-03-19-00-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_04_2 <- read.csv(file = "../data_dumps/data_2022-03-19-08-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_04_3 <- read.csv(file = "../data_dumps/data_2022-03-19-16-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

# Day 5 (Sunday)
data_day_05_1 <- read.csv(file = "../data_dumps/data_2022-03-20-00-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_05_2 <- read.csv(file = "../data_dumps/data_2022-03-20-08-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_05_3 <- read.csv(file = "../data_dumps/data_2022-03-20-16-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

# Day 6 (Monday)
data_day_06_1 <- read.csv(file = "../data_dumps/data_2022-03-21-00-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_06_2 <- read.csv(file = "../data_dumps/data_2022-03-21-08-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_06_3 <- read.csv(file = "../data_dumps/data_2022-03-21-16-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

# Day 7 (Tuesday)
data_day_07_1 <- read.csv(file = "../data_dumps/data_2022-03-22-00-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_07_2 <- read.csv(file = "../data_dumps/data_2022-03-22-08-06.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 
data_day_07_3 <- read.csv(file = "../data_dumps/data_2022-03-22-16-07.csv", 
                          header = T, sep = ",", encoding = "UTF-8") 

data_pulls <- list(data_day_01_1, data_day_01_2, data_day_01_3, 
                   data_day_02_1, data_day_02_2, data_day_02_3,
                   data_day_03_1, data_day_03_2, data_day_03_3, 
                   data_day_04_1, data_day_04_2, data_day_04_3, 
                   data_day_05_1, data_day_05_2, data_day_05_3, 
                   data_day_06_1, data_day_06_2, data_day_06_3,
                   data_day_07_1, data_day_07_2, data_day_07_3) 


subreddits <- c(  "funny",          # 1
                  "AskReddit",      # 2
                  "gaming",         # 3
                  "aww",            # 4
                  "music",          # 5
                  "pics",           # 6
                  "worldnews",      # 7
                  "movies",         # 8
                  "science",        # 9
                  "todayilearned")  # 10



# Create identifier tags as new column Day #1 Pull #2
days <- c(rep(1,3), rep(2,3), rep(3,3), rep(4,3), rep(5,3), rep(6,3), rep(7,3))
times <- c(rep(c(1,2,3),7))
for(i in 1:21){
    data_pulls[[i]] <- data_pulls[[i]] %>% 
        mutate(id_tag = paste0("D", days[i], "P", times[i]))
}

# Data frames binded by day
df_day_1 <- data_frame_bind(data.frame(), data_pulls, 1)
df_day_2 <- data_frame_bind(data.frame(), data_pulls, 4)
df_day_3 <- data_frame_bind(data.frame(), data_pulls, 7)
df_day_4 <- data_frame_bind(data.frame(), data_pulls, 10)
df_day_5 <- data_frame_bind(data.frame(), data_pulls, 13)
df_day_6 <- data_frame_bind(data.frame(), data_pulls, 16)
df_day_7 <- data_frame_bind(data.frame(), data_pulls, 19)


# Filter outpost with less than 30 comments
# Each data frame now has between 113-124 unique posts remaining
df_day_1_filtered <- 
    df_day_1 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30) 
    
df_day_2_filtered <- 
    df_day_2 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30)

df_day_3_filtered <- 
    df_day_3 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30)

df_day_4_filtered <- 
    df_day_4 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30)

df_day_5_filtered <- 
    df_day_5 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30)

df_day_6_filtered <- 
    df_day_6 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30)

df_day_7_filtered <- 
    df_day_7 %>% 
    calculate_number_of_comments() %>% 
    filter(comment_count >= 30)

# Number of posts remaining
# dim(df_day_1_filtered) # 124
# dim(df_day_2_filtered) # 118
# dim(df_day_3_filtered) # 113
# dim(df_day_4_filtered) # 117
# dim(df_day_5_filtered) # 117
# dim(df_day_6_filtered) # 118
# dim(df_day_7_filtered) # 119

# Minimum number of unique posts is 4 (music, Sunday)
unique_post_count_1 <- count_unique_posts(df_day_1_filtered)
unique_post_count_2 <- count_unique_posts(df_day_2_filtered)
unique_post_count_3 <- count_unique_posts(df_day_3_filtered)
unique_post_count_4 <- count_unique_posts(df_day_4_filtered)
unique_post_count_5 <- count_unique_posts(df_day_5_filtered)
unique_post_count_6 <- count_unique_posts(df_day_6_filtered)
unique_post_count_7 <- count_unique_posts(df_day_7_filtered)


# Master data frame of all data pulls
master_data <- data.frame()
for(i in 1:21){
    master_data <- rbind(master_data, data_pulls[[i]])
}


# Count the number of unique comment authors
comment_author_count_df <- 
    master_data %>% 
    group_by(comment_author) %>% 
    mutate(comment_author_count = n()) %>% 
    ungroup() %>% 
    select(comment_author, comment_author_count) %>% 
    distinct() %>% 
    mutate(power_user = ifelse(is_outlier(comment_author_count), TRUE, FALSE))  %>% 
    arrange(desc(comment_author_count))

# Remove extraneous first row
comment_author_count_df <- comment_author_count_df[-1,]


# Top 10 most frequent commenters over the entire week
comment_author_count_df %>% head(10)
```

\newpage

```{r}
# We see that we have 137,074 distinct users
# comment_author_count_df %>% distinct()

# Isolate power users (6,342)
power_users <- 
    comment_author_count_df %>% 
    filter(power_user == TRUE) %>% 
    select(comment_author)

total_author_count <- 137074
power_user_count <- 6342

# power_user_proportion (Approximately 4.63 % of commenters are power users in total data set)
power_user_count / total_author_count
```

# Sampling

```{r}
set.seed(54321)
# Randomly select 30 comments from each subreddit:
df_sampled_comments <- data.frame(subreddit = character(),
                                post_name = character(),
                                karma = integer(),
                                karma_ratio = numeric(),
                                comment_author = character(),
                                comment = character(),
                                comment_karma = numeric(),
                                comment_score = numeric(),
                                id_tag = character())

# Randomly sample 30 comments from each subReddit
df_day_1_comments <- sample_daily_dfs(df_day_1, df_sampled_comments)
df_day_2_comments <- sample_daily_dfs(df_day_2, df_sampled_comments)
df_day_3_comments <- sample_daily_dfs(df_day_3, df_sampled_comments)
df_day_4_comments <- sample_daily_dfs(df_day_4, df_sampled_comments)
df_day_5_comments <- sample_daily_dfs(df_day_5, df_sampled_comments)
df_day_6_comments <- sample_daily_dfs(df_day_6, df_sampled_comments)
df_day_7_comments <- sample_daily_dfs(df_day_7, df_sampled_comments)

# Rename columns
names(df_day_1_comments) <- names(df_sampled_comments)
names(df_day_2_comments) <- names(df_sampled_comments)
names(df_day_3_comments) <- names(df_sampled_comments)
names(df_day_4_comments) <- names(df_sampled_comments)
names(df_day_5_comments) <- names(df_sampled_comments)
names(df_day_6_comments) <- names(df_sampled_comments)
names(df_day_7_comments) <- names(df_sampled_comments)

```



# Feature Engineering

```{r}
# Vectorize list of power users
power_users_vector <- power_users[[1]] %>% as.vector()

# Sentiment bins and power user Boolean columns
df_day_1_comments <- df_day_1_comments %>% feature_engineering() 
df_day_2_comments <- df_day_2_comments %>% feature_engineering()    
df_day_3_comments <- df_day_3_comments %>% feature_engineering() 
df_day_4_comments <- df_day_4_comments %>% feature_engineering() 
df_day_5_comments <- df_day_5_comments %>% feature_engineering() 
df_day_6_comments <- df_day_6_comments %>% feature_engineering() 
df_day_7_comments <- df_day_7_comments %>% feature_engineering() 
```

# Plotting by day

```{r, warning=FALSE}

p1 <- boxplot_plotting_with_dots(df_day_1_comments, "Day 1")
p2 <- boxplot_plotting_with_dots(df_day_2_comments, "Day 2")
p3 <- boxplot_plotting_with_dots(df_day_3_comments, "Day 3")
p4 <- boxplot_plotting_with_dots(df_day_4_comments, "Day 4") 
p5 <- boxplot_plotting_with_dots(df_day_5_comments, "Day 5")
p6 <- boxplot_plotting_with_dots(df_day_6_comments, "Day 6")
p7 <- boxplot_plotting_with_dots(df_day_7_comments, "Day 7")

p1
p2
p3
p4
p5
p6
p7
```



# Compile replications into data frame

```{r}
df_comments_all_days <- list(df_day_1_comments, df_day_2_comments,
                             df_day_3_comments, df_day_4_comments,
                             df_day_5_comments, df_day_6_comments,
                             df_day_7_comments)
```

# All observations and one data from

```{r}
all_days_df <- data.frame()
for(i in 1:7){
  all_days_df <- rbind(all_days_df, df_comments_all_days[[i]])
}

all_data_plot <- all_days_df %>% boxplot_plotting_with_dots()
```

# Check if the power users are significantly different in sentiment from normal users

```{r}
# Calculate the number of power users in the sample data set
# all_days_df # 2100 obvs

# 2,048 Distinct commenters
# 460 comments are from power users
# all_days_df %>% 
#   select(comment_author, is_power_user) %>% 
#   distinct() %>% 
#   filter(is_power_user == TRUE) 

460/2048 # Approximately 22.5% of comments in the sample data set are from power users

# Determine if there is a noticeable difference between power users and nonpower users in terms of sentiment


all_days_df_power_users_true <- 
all_days_df %>% 
  filter(is_power_user == TRUE) 

all_days_df_power_users_false <- 
all_days_df %>% 
  filter(is_power_user == FALSE) 


average_sentiments_true <- 
  all_days_df_power_users_true %>% 
  select(subreddit, sentiment_score) %>% 
  group_by(subreddit) %>% 
  mutate(average_sentiment = mean(sentiment_score)) %>% 
  ungroup() %>% 
  select(subreddit, average_sentiment) %>% 
  distinct()

average_sentiments_false <- 
  all_days_df_power_users_false %>% 
  select(subreddit, sentiment_score) %>% 
  group_by(subreddit) %>% 
  mutate(average_sentiment = mean(sentiment_score)) %>% 
  ungroup() %>% 
  select(subreddit, average_sentiment) %>% 
  distinct()
```

# Graph power users and nonpower user box plots

```{r}

pu_true_plot <- 
  average_sentiments_true %>% 
  ggplot(aes(y = average_sentiment)) + 
  geom_boxplot(fill = "Aquamarine3") +
  labs(
    title    = "Power Users",
    x        = "Power User = True",
    y        = "Average Sentiments"
  ) + theme_tq() + ylim(-1,1)
  
pu_false_plot <- 
average_sentiments_false %>% 
  ggplot(aes(y = average_sentiment)) + 
  geom_boxplot(fill = "Skyblue3") + 
  labs(
    title    = "Non-Power Users",
    x        = "Power User = FALSE",
    y        = "Average Sentiments",
    ) + theme_tq() + ylim(-1,1)

power_user_double_plot <- grid.arrange(pu_true_plot, pu_false_plot, ncol = 2)
```


# Power users t-test

```{r}
# Vectorize average sentiment scores for each subReddit
true_sentiments <- average_sentiments_true[[2]]
false_sentiments <- average_sentiments_false[[2]]


# Variance is equal 
var.test(true_sentiments, false_sentiments)

# Test normality ( Both are normal )
shapiro.test(true_sentiments)
shapiro.test(false_sentiments)

# We do not reject the null hypothesis, power users do not matter
t.test(true_sentiments, false_sentiments, var.equal = TRUE)

```

# Creating seven-day average

```{r}
funny_means <- c()
AskReddit_means <- c()
gaming_means <- c()
aww_means <- c()
music_means <- c()
pics_means <- c()
worldnews_means <- c()
movies_means <- c()
science_means <- c()
todayilearned_means <- c()

# List of means sentiment for each sub Reddit over 
means_list <- list(funny_means,	AskReddit_means,
                   gaming_means, aww_means, music_means,
                   pics_means, worldnews_means, movies_means,
                   science_means, todayilearned_means)


# Isolate each sub Reddit in each data frame
# Calculate the average sentiment 
# and store in the corresponding means_list
for(i in 1:7){
  curr_df <- df_comments_all_days[[i]]
  for(j in 1:10){
    curr_df_2 <- curr_df %>% 
      filter(subreddit == subreddits[j]) %>% 
      mutate(average_sentiment = mean(sentiment_score)) %>% 
      select(average_sentiment) %>% 
      distinct()
      means_list[[j]][i] <- curr_df_2[[1]]
  }
}

# Name columns
names(means_list) <- subreddits

# Converted data frame
means_df <- means_list %>% as.data.frame()
```

# Normality check

```{r}
# Test every column for normality (all columns pass)
for(i in 1:7){
  s <- shapiro.test(means_df[,i])
  print(s)
}
```

# Master Visualization

```{r, warning=FALSE, message=FALSE}
melt_means <- means_df %>% 
  data.table::melt(quietly = TRUE) %>% 
  rename(subreddit = variable, average_sentiment = value)

final_plot <- melt_means %>%
  ggplot(aes(x = subreddit, y = average_sentiment, group = subreddit)) + 
  geom_boxplot(aes(fill = subreddit, alpha = 0.4), show.legend = FALSE, 
               size = .3, colour = "black") +
  stat_summary(fun = "mean") +
  geom_jitter(color = "black", fill = "white", size = 1, alpha = 0.15) + 
  labs(
    title    = "7 Day Average Sentiment Scores",
    subtitle = "Separated by subreddit",
    caption  = "The black dot represents the average",
    x        = "Subreddit",
    y        = "Average Sentiment",
  ) + 
  theme_tq() + 
  theme(axis.text.x   = element_text(angle = 45, vjust = 1, hjust=1, size = 12, face = "bold"),
        axis.title.x  = element_text(size = 12, face = "bold", vjust = 10),
        axis.title.y  = element_text(size = 12, face = "bold")) + 
  ylim(-1,1)
```

# ANOVA

```{r}
# Basic
means_aov <- aov(average_sentiment ~ factor(subreddit), data = melt_means) 
summary(means_aov)
shapiro.test(means_aov$residuals)
model_adequacy <- model_tests(means_aov, 70)
```

# Tukey Test

```{r}
tukey_test <- TukeyHSD(means_aov) 
tukey_df <- tukey_test$`factor(subreddit)` %>% 
  as.data.frame()

tukey_df_filter <- 
  tukey_df %>% 
  filter(`p adj` <= 0.05)

final_tukey_table <- tukey_df_filter %>% knitr::kable(align = c("c", "c", "c", "c")) 
final_tukey_table
```


# Additional models

```{r}
# Blocking
all_days_df_aov <- aov(sentiment_score ~ 
                       factor(subreddit) + 
                       factor(id_tag) + 
                       factor(is_power_user),
                       data = all_days_df)
summary(all_days_df_aov)
# Model not valid
model_tests(all_days_df_aov, 2100)
```


```{r}
# Blocking And interactions
all_days_df_aov_interactions <- 
                       aov(sentiment_score ~ 
                       factor(subreddit) * 
                       factor(id_tag) *  
                       factor(is_power_user),
                       data = all_days_df)
summary(all_days_df_aov_interactions)
# Model not valid
model_tests(all_days_df_aov_interactions, 2100)
```

